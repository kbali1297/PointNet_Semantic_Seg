{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Shape Parts Segmentation using PointNet\n",
    "\n",
    "Here we will train Pointnet to perform point-wise semantic segmentation on the ShapeNet dataset. The semantically labelled point cloud data was taken from \n",
    "https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/pc_seg_plane.png\" alt=\"airplane\" style=\"width: 500px;\"/><img src=\"src/images/pc_seg_bike.png\" alt=\"bike\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Pre-requisites\n",
    "\n",
    "Install required tools for the notebook here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "The model architecture of PointNet is visualized below:\n",
    "\n",
    "\n",
    "<img src=\"src/images/pointnet.png\" alt=\"pointnet_architecture\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now go one step further: We do not just want to learn the overall class label for a given shape but instead for each point in a shape the part it belongs to. We call this Part Segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the ShapeNetPart dataset\n",
    "\n",
    "In terms of data layout, the general idea of shape class identifiers and shape IDs is the same; we just have slightly different shape categories now. Also, each point cloud now has a correponding file specifying the part class for every point.\n",
    "\n",
    "We put the shape class labels for this dataset in `PointNet_Segmentation/data/shape_parts_info.json`, analogous to `shape_info.json` from exercise parts 2.3 and 2.4.\n",
    "\n",
    "The point cloud data is stored as pts files which is basically an even simpler version of obj. It omits the v in front of each line that represents a point and does not support faces. Each line therefore represents one point with its xyz coordinates, separated by a space.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0\n",
    "\n",
    "02691156/                                         # Shape category folder with all its shapes\n",
    "    ├── points                                    # All point clouds go here\n",
    "        ├── 1a04e3eab45ca15dd86060f189eb133.pts   # Point cloud data\n",
    "        ├── 1a32f10b20170883663e90eaf6b4ca52.pts  # Another point cloud\n",
    "        :\n",
    "        :\n",
    "    ├── points_label                              # Part labels for each point in the corresponding pts file\n",
    "        ├── 1a04e3eab45ca15dd86060f189eb133.seg   # Each line represents the local part class of a point\n",
    "        ├── 1a32f10b20170883663e90eaf6b4ca52.seg  # Another segmentation file\n",
    "        :\n",
    "        :\n",
    "    ├── seg_img                                   # Visualizations of the original mesh part segmentation\n",
    "02773838/                                         # Another shape category folder\n",
    "02954340/                                         # In total you should have 16 shape category folders\n",
    ":\n",
    ":\n",
    "train_test_split/                                 # Official split IDs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "--2024-07-28 15:11:50--  https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
      "Resolving shapenet.cs.stanford.edu (shapenet.cs.stanford.edu)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘shapenet.cs.stanford.edu’\n",
      "Extracting ...\n",
      "unzip:  cannot find or open exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip, exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip.zip or exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip.ZIP.\n",
      "rm: cannot remove 'exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip': No such file or directory\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading ...')\n",
    "!wget https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate -P exercise_2/data\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip -d exercise_2/data\n",
    "!rm exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 12137\n",
      "Length of val set: 1870\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.data.shapenet_parts import ShapeNetParts\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNetParts('train')\n",
    "val_dataset = ShapeNetParts('val')\n",
    "overfit_dataset = ShapeNetParts('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 12137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 1870\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the PointNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                                    | Type                 | Params \n",
      "-----------------------------------------------------------------------------------\n",
      "0  | encoder                                 | PointNetEncoder      | 2803529\n",
      "1  | encoder.conv1                           | Conv1d               | 256    \n",
      "2  | encoder.bn1                             | BatchNorm1d          | 128    \n",
      "3  | encoder.conv2                           | Conv1d               | 8320   \n",
      "4  | encoder.bn2                             | BatchNorm1d          | 256    \n",
      "5  | encoder.conv3                           | Conv1d               | 132096 \n",
      "6  | encoder.bn3                             | BatchNorm1d          | 2048   \n",
      "7  | encoder.input_transform_net             | TNet                 | 803081 \n",
      "8  | encoder.input_transform_net.conv_1      | Conv1d               | 256    \n",
      "9  | encoder.input_transform_net.bn_conv_1   | BatchNorm1d          | 128    \n",
      "10 | encoder.input_transform_net.conv_2      | Conv1d               | 8320   \n",
      "11 | encoder.input_transform_net.bn_conv_2   | BatchNorm1d          | 256    \n",
      "12 | encoder.input_transform_net.conv_3      | Conv1d               | 132096 \n",
      "13 | encoder.input_transform_net.bn_conv_3   | BatchNorm1d          | 2048   \n",
      "14 | encoder.input_transform_net.linear_1    | Linear               | 524800 \n",
      "15 | encoder.input_transform_net.bn_lin_1    | BatchNorm1d          | 1024   \n",
      "16 | encoder.input_transform_net.linear_2    | Linear               | 131328 \n",
      "17 | encoder.input_transform_net.bn_lin_2    | BatchNorm1d          | 512    \n",
      "18 | encoder.input_transform_net.linear_3    | Linear               | 2313   \n",
      "19 | encoder.feature_transform_net           | TNet                 | 1857344\n",
      "20 | encoder.feature_transform_net.conv_1    | Conv1d               | 4160   \n",
      "21 | encoder.feature_transform_net.bn_conv_1 | BatchNorm1d          | 128    \n",
      "22 | encoder.feature_transform_net.conv_2    | Conv1d               | 8320   \n",
      "23 | encoder.feature_transform_net.bn_conv_2 | BatchNorm1d          | 256    \n",
      "24 | encoder.feature_transform_net.conv_3    | Conv1d               | 132096 \n",
      "25 | encoder.feature_transform_net.bn_conv_3 | BatchNorm1d          | 2048   \n",
      "26 | encoder.feature_transform_net.linear_1  | Linear               | 524800 \n",
      "27 | encoder.feature_transform_net.bn_lin_1  | BatchNorm1d          | 1024   \n",
      "28 | encoder.feature_transform_net.linear_2  | Linear               | 131328 \n",
      "29 | encoder.feature_transform_net.bn_lin_2  | BatchNorm1d          | 512    \n",
      "30 | encoder.feature_transform_net.linear_3  | Linear               | 1052672\n",
      "31 | conv0                                   | Conv1d               | 557568 \n",
      "32 | bn0                                     | BatchNorm1d          | 1024   \n",
      "33 | conv1                                   | Conv1d               | 131328 \n",
      "34 | bn1                                     | BatchNorm1d          | 512    \n",
      "35 | conv2                                   | Conv1d               | 32896  \n",
      "36 | bn2                                     | BatchNorm1d          | 256    \n",
      "37 | conv3                                   | Conv1d               | 6450   \n",
      "38 | TOTAL                                   | PointNetSegmentation | 3533563\n",
      "Output tensor shape:  torch.Size([8, 1024, 50])\n",
      "Number of traininable params: 3.53M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.model.pointnet import PointNetSegmentation\n",
    "from src.util.model import summarize_model\n",
    "\n",
    "pointnet = PointNetSegmentation(50)\n",
    "print(summarize_model(pointnet))  # Expected: Rows 0-40 and TOTAL = 3533563\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 1024)\n",
    "predictions = pointnet(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: 8, 1024, 50\n",
    "num_trainable_params = sum(p.numel() for p in pointnet.parameters() if p.requires_grad) / 1e6\n",
    "print(f'Number of traininable params: {num_trainable_params:.2f}M')  # Expected: ~3M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script and Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[049/00001] train_loss: 1.034\n",
      "[049/00001] val_loss: 0.377, val_accuracy: 88.017%, val_iou: 0.749\n",
      "[099/00001] train_loss: 0.361\n",
      "[099/00001] val_loss: 0.252, val_accuracy: 90.613%, val_iou: 0.785\n",
      "[149/00001] train_loss: 0.222\n",
      "[149/00001] val_loss: 0.185, val_accuracy: 93.692%, val_iou: 0.837\n",
      "[199/00001] train_loss: 0.162\n",
      "[199/00001] val_loss: 0.162, val_accuracy: 93.980%, val_iou: 0.853\n",
      "[249/00001] train_loss: 0.130\n",
      "[249/00001] val_loss: 0.115, val_accuracy: 95.808%, val_iou: 0.898\n",
      "[299/00001] train_loss: 0.111\n",
      "[299/00001] val_loss: 0.152, val_accuracy: 94.737%, val_iou: 0.867\n",
      "[349/00001] train_loss: 0.193\n",
      "[349/00001] val_loss: 0.132, val_accuracy: 94.987%, val_iou: 0.882\n",
      "[399/00001] train_loss: 0.123\n",
      "[399/00001] val_loss: 0.096, val_accuracy: 96.445%, val_iou: 0.919\n",
      "[449/00001] train_loss: 0.104\n",
      "[449/00001] val_loss: 0.087, val_accuracy: 96.519%, val_iou: 0.928\n",
      "[499/00001] train_loss: 0.106\n",
      "[499/00001] val_loss: 0.074, val_accuracy: 97.066%, val_iou: 0.936\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_pointnet_segmentation\n",
    "config = {\n",
    "    'experiment_name': 'pointnet_segmentation_overfitting',\n",
    "    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,                   # True since we're doing overfitting\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,#f'./src/runs/pointnet_segmentation_overfitting/model_best.ckpt',\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 100,\n",
    "}\n",
    "\n",
    "train_pointnet_segmentation.main(config)  # should be able to get <0.1 loss, >97% accuracy, >0.95 iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training over the entire training set\n",
    "\n",
    "Once your overfitting completes successfully, you can move on to training on the entire dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00099] train_loss: 1.575\n",
      "[000/00199] train_loss: 0.898\n",
      "[000/00249] val_loss: 0.652, val_accuracy: 80.119%, val_iou: 0.672\n",
      "[000/00299] train_loss: 0.737\n",
      "[000/00399] train_loss: 0.716\n",
      "[001/00065] train_loss: 0.609\n",
      "[001/00065] val_loss: 0.745, val_accuracy: 78.263%, val_iou: 0.667\n",
      "[001/00165] train_loss: 0.523\n",
      "[001/00265] train_loss: 0.502\n",
      "[001/00315] val_loss: 0.426, val_accuracy: 85.878%, val_iou: 0.754\n",
      "[001/00365] train_loss: 0.430\n",
      "[002/00031] train_loss: 0.432\n",
      "[002/00131] train_loss: 0.393\n",
      "[002/00131] val_loss: 0.378, val_accuracy: 87.405%, val_iou: 0.770\n",
      "[002/00231] train_loss: 0.389\n",
      "[002/00331] train_loss: 0.393\n",
      "[002/00381] val_loss: 0.323, val_accuracy: 89.085%, val_iou: 0.789\n",
      "[002/00431] train_loss: 0.377\n",
      "[003/00097] train_loss: 0.358\n",
      "[003/00197] train_loss: 0.327\n",
      "[003/00197] val_loss: 0.302, val_accuracy: 89.862%, val_iou: 0.812\n",
      "[003/00297] train_loss: 0.358\n",
      "[003/00397] train_loss: 0.334\n",
      "[004/00013] val_loss: 0.274, val_accuracy: 90.782%, val_iou: 0.820\n",
      "[004/00063] train_loss: 0.306\n",
      "[004/00163] train_loss: 0.315\n",
      "[004/00263] train_loss: 0.312\n",
      "[004/00263] val_loss: 0.291, val_accuracy: 90.056%, val_iou: 0.818\n",
      "[004/00363] train_loss: 0.333\n",
      "[005/00029] train_loss: 0.330\n",
      "[005/00079] val_loss: 0.312, val_accuracy: 88.860%, val_iou: 0.816\n",
      "[005/00129] train_loss: 0.317\n",
      "[005/00229] train_loss: 0.319\n",
      "[005/00329] train_loss: 0.289\n",
      "[005/00329] val_loss: 0.287, val_accuracy: 90.345%, val_iou: 0.826\n",
      "[005/00429] train_loss: 0.304\n"
     ]
    }
   ],
   "source": [
    "from src.training import train_pointnet_segmentation\n",
    "config = {\n",
    "    'experiment_name': 'pointnet_segmentation_generalization',\n",
    "    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 28,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 6,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "}\n",
    "\n",
    "train_pointnet_segmentation.main(config)  # Should be able to get > 90% accuracy and > 0.8 iou on the val set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.infer_pointnet_segmentation import InferenceHandlerPointNetSegmentation\n",
    "from src.util.visualization import visualize_pointcloud\n",
    "from matplotlib import cm, colors\n",
    "import numpy as np\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandlerPointNetSegmentation('src/runs/pointnet_segmentation_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34960/3149760081.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2960dab46144e4a57466b5fb5cc264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get shape point cloud, predict labels, and visualize colored point cloud\n",
    "shapenet_obj = ShapeNetParts('val')\n",
    "shape_points = shapenet_obj.get_point_cloud_with_labels('02691156/1c4b8662938adf41da2b0f839aba40f9')[0]\n",
    "point_labels = inferer.infer_single(shape_points)\n",
    "point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n",
    "point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n",
    "point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n",
    "visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34960/368684140.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2909b55f6b2d4c6a882df9adc69a7d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get shape point cloud, predict labels, and visualize colored point cloud\n",
    "shape_points = shapenet_obj.get_point_cloud_with_labels('03948459/e017cf5dac1e39b013d74211a209ce')[0]\n",
    "point_labels = inferer.infer_single(shape_points)\n",
    "point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n",
    "point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n",
    "point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n",
    "visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34960/1178286923.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ce6bbb43054b0ea0646a03a6197a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get shape point cloud, predict labels, and visualize colored point cloud\n",
    "shape_points = shapenet_obj.get_point_cloud_with_labels('03790512/86b6dc954e1ca8e948272812609617e2')[0]\n",
    "point_labels = inferer.infer_single(shape_points)\n",
    "point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n",
    "point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n",
    "point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n",
    "visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Qi, C. et al. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 77-85."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
